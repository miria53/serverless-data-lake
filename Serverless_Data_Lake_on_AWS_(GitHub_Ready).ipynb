{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Serverless Data Lake on AWS (GitHub Ready)\n",
        "\n",
        "# This immersive provides a conceptual implementation for a serverless data lake on AWS.\n",
        "# It demonstrates how components like S3, Lambda, Glue, and Athena can be integrated\n",
        "# and managed through a GitHub-driven deployment.\n",
        "\n",
        "# Project Overview:\n",
        "# This project establishes a serverless data lake architecture designed to ingest,\n",
        "# process, and query data efficiently. Raw data is landed in an S3 bucket, processed\n",
        "# by AWS Glue for ETL, and made queryable via AWS Athena. AWS Lambda functions handle\n",
        "# event-driven processing, such as triggering Glue jobs upon new data arrival.\n",
        "\n",
        "# GitHub Repository Structure Suggestion:\n",
        "# Your GitHub repository for this project might look like this:\n",
        "# my-data-lake/\n",
        "# ├── lambda_functions/\n",
        "# │   └── data_ingestion_trigger/\n",
        "# │       ├── lambda_function.py   # Python code for Lambda\n",
        "# │       └── requirements.txt     # Lambda dependencies\n",
        "# ├── glue_scripts/\n",
        "# │   └── transform_data.py        # Python/PySpark script for Glue ETL\n",
        "# ├── cloudformation/\n",
        "# │   └── data_lake_stack.yml      # CloudFormation template for infrastructure\n",
        "# ├── data/\n",
        "# │   └── raw_data_example.csv     # Example raw data\n",
        "# └── README.md\n",
        "\n",
        "# --- AWS Lambda Function Example (lambda_functions/data_ingestion_trigger/lambda_function.py) ---\n",
        "# This Lambda function is triggered by S3 object creation events and starts an AWS Glue job.\n",
        "\n",
        "import json\n",
        "import os\n",
        "import boto3\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    \"\"\"\n",
        "    Lambda function to trigger an AWS Glue job when a new object is uploaded to an S3 bucket.\n",
        "    \"\"\"\n",
        "    print(\"Received event:\", json.dumps(event))\n",
        "\n",
        "    s3_bucket = event['Records'][0]['s3']['bucket']['name']\n",
        "    s3_key = event['Records'][0]['s3']['object']['key']\n",
        "\n",
        "    print(f\"New object '{s3_key}' detected in bucket '{s3_bucket}'.\")\n",
        "\n",
        "    glue_client = boto3.client('glue')\n",
        "    glue_job_name = os.environ.get('GLUE_JOB_NAME', 'your-glue-etl-job') # Set this as an env var in Lambda config\n",
        "\n",
        "    try:\n",
        "        response = glue_client.start_job_run(\n",
        "            JobName=glue_job_name,\n",
        "            Arguments={\n",
        "                '--S3_INPUT_BUCKET': s3_bucket,\n",
        "                '--S3_INPUT_KEY': s3_key,\n",
        "                '--S3_OUTPUT_LOCATION': os.environ.get('S3_PROCESSED_DATA_BUCKET', 's3://your-processed-data-bucket/processed/')\n",
        "            }\n",
        "        )\n",
        "        print(f\"Successfully started Glue job '{glue_job_name}'. Run ID: {response['JobRunId']}\")\n",
        "        return {\n",
        "            'statusCode': 200,\n",
        "            'body': json.dumps(f\"Glue job '{glue_job_name}' triggered successfully.\")\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error triggering Glue job: {e}\")\n",
        "        return {\n",
        "            'statusCode': 500,\n",
        "            'body': json.dumps(f\"Error triggering Glue job: {str(e)}\")\n",
        "        }\n",
        "\n",
        "# --- AWS Glue ETL Script Example (glue_scripts/transform_data.py) ---\n",
        "# This PySpark script reads data from S3, performs a simple transformation, and writes it back to S3.\n",
        "\n",
        "\"\"\"\n",
        "import sys\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "\n",
        "# @params: [JOB_NAME]\n",
        "args = getResolvedOptions(sys.argv, [\n",
        "    'JOB_NAME',\n",
        "    '--S3_INPUT_BUCKET',\n",
        "    '--S3_INPUT_KEY',\n",
        "    '--S3_OUTPUT_LOCATION'\n",
        "])\n",
        "\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "job = Job(glueContext)\n",
        "job.init(args['JOB_NAME'], args)\n",
        "\n",
        "s3_input_bucket = args['S3_INPUT_BUCKET']\n",
        "s3_input_key = args['S3_INPUT_KEY']\n",
        "s3_output_location = args['S3_OUTPUT_LOCATION']\n",
        "\n",
        "print(f\"Reading data from s3://{s3_input_bucket}/{s3_input_key}\")\n",
        "\n",
        "# Read raw data from S3 (e.g., CSV)\n",
        "datasource = glueContext.create_dynamic_frame.from_options(\n",
        "    connection_type=\"s3\",\n",
        "    connection_options={\"paths\": [f\"s3://{s3_input_bucket}/{s3_input_key}\"]},\n",
        "    format=\"csv\",\n",
        "    format_options={\"withHeader\": True, \"separator\": \",\"},\n",
        "    transformation_ctx=\"datasource_raw\"\n",
        ")\n",
        "\n",
        "# Example Transformation: Convert to a different format (e.g., Parquet) and add a timestamp\n",
        "print(\"Applying transformations...\")\n",
        "transformed_df = datasource.toDF()\n",
        "transformed_df = transformed_df.withColumn(\"processing_timestamp\", current_timestamp())\n",
        "\n",
        "# Write processed data back to S3 in Parquet format\n",
        "print(f\"Writing processed data to {s3_output_location}\")\n",
        "glueContext.write_dynamic_frame.from_options(\n",
        "    frame=DynamicFrame.fromDF(transformed_df, glueContext, \"transformed_df\"),\n",
        "    connection_type=\"s3\",\n",
        "    connection_options={\"path\": s3_output_location},\n",
        "    format=\"parquet\",\n",
        "    transformation_ctx=\"datasink_processed\"\n",
        ")\n",
        "\n",
        "job.commit()\n",
        "print(\"Glue job completed successfully.\")\n",
        "\"\"\"\n",
        "\n",
        "# --- AWS CloudFormation Template Example (cloudformation/data_lake_stack.yml) ---\n",
        "# This template defines the AWS resources for your data lake.\n",
        "# Using Infrastructure as Code (IaC) like CloudFormation (or Terraform) is crucial for GitHub deployment.\n",
        "\n",
        "\"\"\"\n",
        "AWSTemplateFormatVersion: '2010-09-09'\n",
        "Description: A serverless data lake architecture on AWS.\n",
        "\n",
        "Parameters:\n",
        "  RawDataBucketName:\n",
        "    Type: String\n",
        "    Default: your-raw-data-bucket-name-unique\n",
        "    Description: Name for the S3 bucket to store raw data.\n",
        "  ProcessedDataBucketName:\n",
        "    Type: String\n",
        "    Default: your-processed-data-bucket-name-unique\n",
        "    Description: Name for the S3 bucket to store processed data.\n",
        "  GlueJobName:\n",
        "    Type: String\n",
        "    Default: MyDataLakeETLJob\n",
        "    Description: Name for the AWS Glue ETL job.\n",
        "\n",
        "Resources:\n",
        "  # S3 Bucket for Raw Data\n",
        "  RawDataBucket:\n",
        "    Type: AWS::S3::Bucket\n",
        "    Properties:\n",
        "      BucketName: !Ref RawDataBucketName\n",
        "      Tags:\n",
        "        - Key: Project\n",
        "          Value: DataLake\n",
        "      NotificationConfiguration:\n",
        "        LambdaConfigurations:\n",
        "          - Event: s3:ObjectCreated:*\n",
        "            Function: !GetAtt GlueTriggerLambdaFunction.Arn\n",
        "            Filter:\n",
        "              S3Key:\n",
        "                Rules:\n",
        "                  - Name: suffix\n",
        "                    Value: .csv # Trigger only for CSV files\n",
        "\n",
        "  # S3 Bucket for Processed Data\n",
        "  ProcessedDataBucket:\n",
        "    Type: AWS::S3::Bucket\n",
        "    Properties:\n",
        "      BucketName: !Ref ProcessedDataBucketName\n",
        "      Tags:\n",
        "        - Key: Project\n",
        "          Value: DataLake\n",
        "\n",
        "  # IAM Role for Lambda Function\n",
        "  LambdaExecutionRole:\n",
        "    Type: AWS::IAM::Role\n",
        "    Properties:\n",
        "      AssumeRolePolicyDocument:\n",
        "        Version: '2012-10-17'\n",
        "        Statement:\n",
        "          - Effect: Allow\n",
        "            Principal:\n",
        "              Service: lambda.amazonaws.com\n",
        "            Action: sts:AssumeRole\n",
        "      ManagedPolicyArns:\n",
        "        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n",
        "      Policies:\n",
        "        - PolicyName: LambdaS3GlueAccess\n",
        "          PolicyDocument:\n",
        "            Version: '2012-10-17'\n",
        "            Statement:\n",
        "              - Effect: Allow\n",
        "                Action:\n",
        "                  - s3:GetObject\n",
        "                  - s3:ListBucket\n",
        "                Resource: !Join ['', ['arn:aws:s3:::', !Ref RawDataBucketName, '/*']]\n",
        "              - Effect: Allow\n",
        "                Action:\n",
        "                  - s3:PutObject\n",
        "                  - s3:ListBucket\n",
        "                Resource: !Join ['', ['arn:aws:s3:::', !Ref ProcessedDataBucketName, '/*']]\n",
        "              - Effect: Allow\n",
        "                Action:\n",
        "                  - glue:StartJobRun\n",
        "                Resource: !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:job/${GlueJobName}'\n",
        "\n",
        "  # Lambda Function to Trigger Glue Job\n",
        "  GlueTriggerLambdaFunction:\n",
        "    Type: AWS::Lambda::Function\n",
        "    Properties:\n",
        "      FunctionName: !Sub 'data-lake-glue-trigger-${AWS::StackName}'\n",
        "      Handler: lambda_function.lambda_handler\n",
        "      Runtime: python3.9\n",
        "      Code:\n",
        "        S3Bucket: !Ref CodeBucket # Assuming you upload your code to a separate S3 bucket\n",
        "        S3Key: lambda_functions/data_ingestion_trigger.zip\n",
        "      MemorySize: 128\n",
        "      Timeout: 30\n",
        "      Role: !GetAtt LambdaExecutionRole.Arn\n",
        "      Environment:\n",
        "        Variables:\n",
        "          GLUE_JOB_NAME: !Ref GlueJobName\n",
        "          S3_PROCESSED_DATA_BUCKET: !Join ['', ['s3://', !Ref ProcessedDataBucketName, '/processed/']]\n",
        "\n",
        "  # Permission for S3 to invoke Lambda\n",
        "  S3LambdaPermission:\n",
        "    Type: AWS::Lambda::Permission\n",
        "    Properties:\n",
        "      FunctionName: !GetAtt GlueTriggerLambdaFunction.Arn\n",
        "      Action: lambda:InvokeFunction\n",
        "      Principal: s3.amazonaws.com\n",
        "      SourceAccount: !Ref AWS::AccountId\n",
        "      SourceArn: !GetAtt RawDataBucket.Arn\n",
        "\n",
        "  # IAM Role for Glue Job\n",
        "  GlueExecutionRole:\n",
        "    Type: AWS::IAM::Role\n",
        "    Properties:\n",
        "      AssumeRolePolicyDocument:\n",
        "        Version: '2012-10-17'\n",
        "        Statement:\n",
        "          - Effect: Allow\n",
        "            Principal:\n",
        "              Service: glue.amazonaws.com\n",
        "            Action: sts:AssumeRole\n",
        "      ManagedPolicyArns:\n",
        "        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole\n",
        "      Policies:\n",
        "        - PolicyName: GlueS3Access\n",
        "          PolicyDocument:\n",
        "            Version: '2012-10-17'\n",
        "            Statement:\n",
        "              - Effect: Allow\n",
        "                Action:\n",
        "                  - s3:GetObject\n",
        "                  - s3:PutObject\n",
        "                  - s3:ListBucket\n",
        "                Resource:\n",
        "                  - !Join ['', ['arn:aws:s3:::', !Ref RawDataBucketName, '/*']]\n",
        "                  - !Join ['', ['arn:aws:s3:::', !Ref ProcessedDataBucketName, '/*']]\n",
        "              - Effect: Allow\n",
        "                Action:\n",
        "                  - s3:ListBucket\n",
        "                Resource:\n",
        "                  - !Join ['', ['arn:aws:s3:::', !Ref RawDataBucketName]]\n",
        "                  - !Join ['', ['arn:aws:s3:::', !Ref ProcessedDataBucketName]]\n",
        "\n",
        "  # AWS Glue ETL Job\n",
        "  GlueETLJob:\n",
        "    Type: AWS::Glue::Job\n",
        "    Properties:\n",
        "      Name: !Ref GlueJobName\n",
        "      Command:\n",
        "        Name: glueetl\n",
        "        ScriptLocation: !Join ['', ['s3://', !Ref CodeBucket, '/glue_scripts/transform_data.py']] # Assuming code in S3\n",
        "        PythonVersion: 3\n",
        "      Role: !GetAtt GlueExecutionRole.Arn\n",
        "      GlueVersion: '3.0'\n",
        "      WorkerType: Standard\n",
        "      NumberOfWorkers: 2\n",
        "      Timeout: 60\n",
        "\n",
        "  # AWS Glue Data Catalog Database (for Athena)\n",
        "  DataLakeDatabase:\n",
        "    Type: AWS::Glue::Database\n",
        "    Properties:\n",
        "      CatalogId: !Ref AWS::AccountId\n",
        "      DatabaseInput:\n",
        "        Name: my_data_lake_db\n",
        "        Description: Database for the serverless data lake\n",
        "\n",
        "Outputs:\n",
        "  RawDataBucketArn:\n",
        "    Description: ARN of the S3 bucket for raw data\n",
        "    Value: !GetAtt RawDataBucket.Arn\n",
        "  ProcessedDataBucketArn:\n",
        "    Description: ARN of the S3 bucket for processed data\n",
        "    Value: !GetAtt ProcessedDataBucket.Arn\n",
        "  GlueJobNameOutput:\n",
        "    Description: Name of the AWS Glue ETL Job\n",
        "    Value: !Ref GlueJobName\n",
        "  LambdaFunctionArn:\n",
        "    Description: ARN of the Lambda function that triggers the Glue job\n",
        "    Value: !GetAtt GlueTriggerLambdaFunction.Arn\n",
        "\"\"\"\n",
        "\n",
        "# --- AWS Setup and GitHub Integration Steps ---\n",
        "\n",
        "# 1.  **Create an S3 Bucket for Code:**\n",
        "#     * Create an S3 bucket (e.g., `your-data-lake-code-bucket`) to store your Lambda function ZIP files and Glue scripts.\n",
        "#     * This bucket is referenced in the CloudFormation template (`CodeBucket`).\n",
        "\n",
        "# 2.  **Package and Upload Code:**\n",
        "#     * **Lambda Function:**\n",
        "#         * Navigate to `lambda_functions/data_ingestion_trigger/` locally.\n",
        "#         * Install dependencies: `pip install -r requirements.txt -t .`\n",
        "#         * Zip the contents: `zip -r ../data_ingestion_trigger.zip .`\n",
        "#         * Upload `data_ingestion_trigger.zip` to `s3://your-data-lake-code-bucket/lambda_functions/`.\n",
        "#     * **Glue Script:**\n",
        "#         * Upload `glue_scripts/transform_data.py` to `s3://your-data-lake-code-bucket/glue_scripts/`.\n",
        "\n",
        "# 3.  **Create IAM Roles:**\n",
        "#     * The CloudFormation template defines `LambdaExecutionRole` and `GlueExecutionRole` with necessary permissions. When you deploy the CloudFormation stack, these roles will be created.\n",
        "\n",
        "# 4.  **Deploy CloudFormation Stack:**\n",
        "#     * Go to AWS CloudFormation in the AWS Management Console.\n",
        "#     * Click \"Create stack\" -> \"With new resources (standard)\".\n",
        "#     * Upload your `cloudformation/data_lake_stack.yml` file.\n",
        "#     * Provide values for parameters like `RawDataBucketName` and `ProcessedDataBucketName` (ensure they are globally unique).\n",
        "#     * Acknowledge IAM resource creation and create the stack.\n",
        "#     * This will provision your S3 buckets, Lambda function, Glue job, and necessary IAM roles.\n",
        "\n",
        "# 5.  **Set up S3 Event Notification (if not done by CloudFormation):**\n",
        "#     * If your CloudFormation template doesn't automatically configure S3 event notifications (it does in the example above), you would manually configure the `RawDataBucket` to send `ObjectCreated` events to your `GlueTriggerLambdaFunction`.\n",
        "\n",
        "# 6.  **Push to GitHub:**\n",
        "#     * Once your AWS resources are set up (preferably via CloudFormation), commit your entire `my-data-lake` project to your GitHub repository.\n",
        "#     * You can use **GitHub Actions** or **AWS CodePipeline** to automate the deployment of your CloudFormation stack and code updates.\n",
        "\n",
        "#     * **Example GitHub Actions Workflow (.github/workflows/deploy-data-lake.yml):**\n",
        "#         # This is a simplified example. In a real scenario, you'd use OIDC for authentication\n",
        "#         # and more robust actions.\n",
        "#         \"\"\"\n",
        "#         name: Deploy Data Lake\n",
        "#         on:\n",
        "#           push:\n",
        "#             branches:\n",
        "#               - main\n",
        "#             paths:\n",
        "#               - 'cloudformation/**'\n",
        "#               - 'lambda_functions/**'\n",
        "#               - 'glue_scripts/**'\n",
        "\n",
        "#         jobs:\n",
        "#           deploy:\n",
        "#             runs-on: ubuntu-latest\n",
        "#             steps:\n",
        "#               - name: Checkout code\n",
        "#                 uses: actions/checkout@v3\n",
        "\n",
        "#               - name: Configure AWS credentials\n",
        "#                 uses: aws-actions/configure-aws-credentials@v2\n",
        "#                 with:\n",
        "#                   aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
        "#                   aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
        "#                   aws-region: us-east-1 # Replace with your AWS region\n",
        "\n",
        "#               - name: Upload Lambda function code to S3\n",
        "#                 run: |\n",
        "#                   cd lambda_functions/data_ingestion_trigger/\n",
        "#                   pip install -r requirements.txt -t .\n",
        "#                   zip -r ../data_ingestion_trigger.zip .\n",
        "#                   aws s3 cp ../data_ingestion_trigger.zip s3://your-data-lake-code-bucket/lambda_functions/data_ingestion_trigger.zip\n",
        "\n",
        "#               - name: Upload Glue script to S3\n",
        "#                 run: |\n",
        "#                   aws s3 cp glue_scripts/transform_data.py s3://your-data-lake-code-bucket/glue_scripts/transform_data.py\n",
        "\n",
        "#               - name: Deploy CloudFormation stack\n",
        "#                 uses: aws-actions/aws-cloudformation-github-deploy@v1\n",
        "#                 with:\n",
        "#                   name: my-data-lake-stack\n",
        "#                   template: cloudformation/data_lake_stack.yml\n",
        "#                   parameter-overrides: |\n",
        "#                     RawDataBucketName=your-raw-data-bucket-name-unique\n",
        "#                     ProcessedDataBucketName=your-processed-data-bucket-name-unique\n",
        "#                   capabilities: CAPABILITY_IAM,CAPABILITY_NAMED_IAM\n",
        "#         \"\"\"\n",
        "\n",
        "# Workflow:\n",
        "# 1.  A developer pushes new raw data (e.g., `raw_data_example.csv`) to the `RawDataBucket` via S3 console, API, or programmatic upload.\n",
        "# 2.  The S3 `ObjectCreated` event triggers the `GlueTriggerLambdaFunction`.\n",
        "# 3.  The Lambda function starts the `GlueETLJob`, passing the S3 input and output paths as arguments.\n",
        "# 4.  The Glue job reads the raw data, transforms it, and writes the processed data to the `ProcessedDataBucket`.\n",
        "# 5.  Users can then query the processed data using AWS Athena, pointing to the `ProcessedDataBucket` and the `my_data_lake_db` Glue Data Catalog.\n",
        "\n",
        "# Considerations:\n",
        "# * **Schema Evolution:** Implement strategies for handling schema changes in your data, such as using Glue Schema Registry or evolving your Glue ETL scripts.\n",
        "# * **Data Partitioning:** For large datasets, partition your data in S3 (e.g., by date) to optimize Athena query performance and cost.\n",
        "# * **Security:** Ensure proper IAM policies are in place, and data is encrypted at rest and in transit.\n",
        "# * **Monitoring:** Use CloudWatch to monitor Lambda invocations, Glue job runs, and S3 bucket activity."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "5riBTFt5gqLc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}